## Overview
This document explains how AI tools were used during the development of **Modelia-ai-studio-2.0**.  
AI assistance was used to improve productivity, write boilerplate code faster, and debug errors efficiently.  
All code was reviewed and tested manually before committing.

---

## 🧠 Tools Used

### 💻 **Cursor AI**
- Helped with **code autocompletion** while writing backend and frontend logic.  
- Generated initial versions of **test files** (Jest + Vitest).  
- Assisted in **structuring the CI workflow** and organizing the project structure.  
- Greatly improved typing speed and repetitive code generation.

### 🧩 **ChatGPT**
- Used for **debugging failed tests**, fixing validation logic, and improving API structure.  
- Helped in **understanding Jest/Vitest configurations** and **writing clear documentation**.  
- Provided **code review and optimization** suggestions.

### 🌐 **Gemini**
- Supported **error analysis and debugging** when certain edge cases appeared in tests.  
- Gave alternate solutions and performance insights during backend refactoring.

---

## 🧪 How AI Was Used in Testing

- **Cursor AI** generated most of the test cases automatically for backend (Jest) and frontend (Vitest).  
- After generation, all tests were **manually reviewed and executed** to fix failing or inaccurate ones.  
- **ChatGPT and Gemini** were then used to debug and explain failing test behavior.
